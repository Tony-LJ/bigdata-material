
# 离线数仓开发经验整理

---

## 分区表写入技巧
- [业务场景:增量离线同步]()
```.text
1.上游Oracle有一个每日数据量新增很大，整体全量很大的表需要同步到大数据系统(Impala/hive)，并带有数据记录更新字段:update_time
ODS层：考虑到数据量巨大，由全量抽取改为基于update_time字段进行增量抽取；增量抽取条件：update_time >= sysdate-1
CREATE TABLE bi_ods.ods_zrsmt_oms_pdm_yield_record_to_hadoop (
  yield_record_id BIGINT,
  organization_id DOUBLE,
  ebs_mfg_code STRING,
  process_code STRING,
  process_name STRING,
  lot_num STRING,
  sales_order_number STRING,
  test_year_week STRING,
  kw_part_num STRING,
  layer_num STRING,
  layup_num STRING,
  product_sample STRING,
  surface_treatment_name STRING,
  customer_code STRING,
  customer_description STRING,
  kwcolcode STRING,
  line_code STRING,
  test_device_code STRING,
  shift STRING,
  create_by STRING,
  test_by STRING,
  amount_unit STRING,
  test_amount DOUBLE,
  yield_amount DOUBLE,
  test_time STRING,
  create_time STRING,
  test_pnl_amount DOUBLE,
  rework_amount DOUBLE,
  update_time STRING,
  check_method STRING,
  check_type STRING,
  board_type STRING
)COMMENT 'Imported by sqoop on 2025/05/21 06:00:07'
2.从同步性能角度，考虑使用以update_time字段作为增量同步识别字段；
DWD层：
CREATE TABLE bi_dwd.dwd_zrsmt_oms_pdm_yield_record_to_hadoop (
  yield_record_id BIGINT,
  organization_id DOUBLE,
  ebs_mfg_code STRING,
  process_code STRING,
  process_name STRING,
  lot_num STRING,
  sales_order_number STRING,
  test_year_week STRING,
  kw_part_num STRING,
  layer_num STRING,
  layup_num STRING,
  product_sample STRING,
  surface_treatment_name STRING,
  customer_code STRING,
  customer_description STRING,
  kwcolcode STRING,
  line_code STRING,
  test_device_code STRING,
  shift STRING,
  create_by STRING,
  test_by STRING,
  amount_unit STRING,
  test_amount DOUBLE,
  yield_amount DOUBLE,
  test_time STRING,
  create_time STRING,
  test_pnl_amount DOUBLE,
  rework_amount DOUBLE,
  update_time STRING,
  check_method STRING,
  check_type STRING,
  board_type STRING
)
PARTITIONED BY ( pt_d STRING COMMENT '天分区')
comment '工厂过数明细表'
3.根据update_time分区动态写入分区,实现数据表增量同步
insert overwrite table  bi_data.dwd_zrsmt_oms_pdm_yield_record_to_hadoop  
partition (pt_m)
    select 
    yield_record_id,
    organization_id,
    ebs_mfg_code,
    process_code,
    process_name,
    lot_num,
    sales_order_number,
    test_year_week,
    kw_part_num,
    layer_num,
    layup_num,
    product_sample,
    surface_treatment_name,
    customer_code,
    customer_description,
    kwcolcode,
    line_code,
    test_device_code,
    shift,
    create_by,
    test_by,
    amount_unit,
    test_amount,
    yield_amount,
    test_time,
    create_time,
    test_pnl_amount,
    rework_amount,
    update_time,
    check_method,
    check_type,
    board_type,
    to_date(trunc(update_time,'dd'))   as pt_d 
from    bi_ods.ods_cux_cux_mes_data_flow_all
where   to_date(update_time)  >= to_date(trunc(date_sub(now(),1),'dd'));
```






















### 参考日期


